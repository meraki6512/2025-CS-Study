# 캐시 메모리

|  | 주제                          | 요약                                               |
| -- | --------------------------- | -------------------------------------------------- |
| 1  | **캐시 메모리란?**                | CPU와 주기억장치 간 속도 차이 보완                    |
| 2  | **계층 구조**      | L1: 속도↑ 용량↓ ~ L3: 속도↓ 용량↑                      |
| 3  | **지역성(Locality)의 원리**       | 시간/공간                            |
| 4  | **캐시 Miss**                | Compulsory, Conflict, Capacity miss 설명             |
| 5  | **캐시 매핑 방식**                | 직접 매핑, 완전 연관 매핑, 세트 연관 매핑                          |
| 6  | **캐시 교체 알고리즘**              | LRU, FIFO 등 → Miss 시 교체할 블록           |
| 7  | **쓰기 정책 (Write Policy)**    | Write-Through vs Write-Back                    |

</br>

---

## 1. 캐시 메모리란?
> 주로 속도가 빠른 **CPU**와 느린 **RAM**의 **`속도 차이`**에 따른 병목 현상을 보완하기 위한 메모리를 가리킨다.

**원리**: CPU가 RAM에서 저장된 데이터를 읽어올 때, <u>자주 사용할 것 같은</u> 데이터를 캐시 메모리에 저장한 뒤, 다음에 캐시 메모리에서 먼저 가져온다.

**특징**: 캐시 메모리는 RAM보다 `용량이 적고↓` `비용이 비싸다↑`.

</br>

## 2. 계층 구조
> 캐시 메모리는 속도와 크기에 따라 **L1, L2, L3** 캐시 메모리로 분류된다.

|  | 속도 | 용량 | 위치 |
| -- | ------------ |  ------------ |  ------------ | 
| `L1`  | 빠름 ↑ | 작음 ↓ | CPU 내부 |
| `L2`  | - | - | CPU와 RAM 사이 |
| `L3`  | 빠름 ↓ | 큼 ↑ | CPU 외부 (주로 메인보드) |

일반적으로 CPU에서 가장 빠르게 접근 가능한 L1 캐시부터 사용되고, 데이터를 찾지 못하면 L2로 이동하는 방식이다.

듀얼 코어 프로세서에는 각 코어마다 독립된 L1 캐시 메모리와 두 코어가 서로 공유하는 공유형 L2 캐시 메모리가 내장된다.

</br>

## 3. 지역성(Locality)의 원리

> = 참조 지역성의 원리

### 1. `시간 지역성`
- 한 번 참조된 데이터는 다시 참조될 가능성이 높음

### 2. `공간 지역성`
- 연속된 주소의 데이터는 다시 참조될 가능성이 높음
- 참조된 데이터의 주변 데이터는 곧 참조될 가능성이 높음

</br>

## 4. 캐시 Miss
> **Hit**: 데이터 찾음, **Miss**: 못 찾음

CPU가 요청한 데이터가 캐시에 없으면 DRAM에서 가져와야 하는데 이 경우를 캐시 미스가 났다고 한다.

캐시 미스에는 세 가지의 종류가 있다.

### 1. **`Compulsory` (Cold start)** Miss </br>
* 말 그대로 데이터를 <u>가장 처음 접근</u>할 때에는 필연적으로 미스가 발생

### 2. **`Conflict`** Miss
* 캐시 메모리에 저장할 여러 <u>데이터가 같은 주소</u>로 할당되어 있는 경우 미스가 발생 </br>
* (Direct mapping 방식에서 주로 발생 (잘 사용X))

### 3. **`Capacity`** miss
* 캐시 메모리의 <u>공간이 부족</u>한 경우 미스가 발생 </br>
* (크기↑이 해결 방법은 아님: 속도↓ 파워↑)

</br>

## 5. 캐시 매핑 방식

> 캐시 메모리는 메인 메모리에 비해 크기가 매우 작기 때문에 1:1로 매칭할 수 없다.

### 1. **`Direct` (직접)** mapping
* 메모리 주소와 캐시 주소의 **순서를 일치**시켜 저장한다.
* 1-100까지의 메모리 주소와 1-10까지의 캐시 주소가 있다고 가정하자. </br>
1-10 메모리는 1 캐시로 매핑하고,
11-20 메모리는 2 캐시로 매핑하는 것이다.
* <u>Conflict Miss</u>가 잘 발생하고 효율이 떨어지는 단순한 방식 </br>
-> 잘 사용하지 않음

### 2. **`Associative` (연관)** mapping
* 순서를 일치시키지 않고 **어디든** 저장할 수 있다.
* 검색은 느리지만 <u>적중률은 높다</u>. </br>
    (캐시 자체는 속도가 빨라 적중률이 높은 게 낫다.)

### 3. **`Set Associative` (직접 연관)** mapping
* Direct + Associative
* **순서는 일치**시키고, **블록화**를 해 블록 내부에는 어디든 저장할 수 있다.
* <u>검색은 보다 효율적</u>이고, 저장 위치의 주소에 큰 제약이 있는 건 아니기 때문에 <u>적중률도 많이 떨어지지 않는다</u>.

</br>

## 6. 캐시 교체 알고리즘

캐시 메모리의 용량은 매우 작기 때문에 효율적으로 사용해야 한다. **어떤 데이터를 오래 저장할 것이냐**에 대한 다양한 알고리즘이 제시된다.

### 1. LRU (Least Recently Used)
> 가장 최근까지 오래 사용되지 않은 데이터부터 제거
* <u>구현 간단, 보편적</u>
* `TimeStamp`를 사용해 구현하거나 (느림, 오버헤드)
* `Double Linked List`를 사용해 구현할 수 있음 (O(1))

### 2. LFU (Least Frequently Used)
> 가장 사용 횟수가 적은 데이터부터 제거
* 사용 횟수가 일치하면 LRU 알고리즘에 따름
* 단점
* (1) 최근 적재된 데이터가 <u>금방 교체</u>될 수 있음
* (2) 초반에 집중적으로 쓰이고, 이후 안 쓰인 데이터가 계속 메모리에 남아있어, <u>메모리를 낭비</u>할 가능성이 있음

### 3. FIFO (First In First Out)
> 메모리에 올라온 순서대로 제거
* <u>구현 간단</u>
* `Queue`

</br>

## 7. 쓰기 정책
> **속도**와 **일관성**의 Trade Off

"캐시 변경 내용을 언제 메인 메모리에 반영할까?"에 관한 전략이다.

### 1. Write-Through (통과)

캐시와 메모리에 `동시에` 기록 </br>
(캐시에 쓰자마자 메모리에도 씀)
- **일관성 좋음**
- **속도 느림↓** (메모리 접근 많음)

### 2. Wrtie-Back (뒤)

일단 캐시에만 기록, 메모리는 `나중에` 반영 </br>
(캐시 블록이 교체될 때만 메모리에 저장)
- **일관성 유지가 어려움** (추가 제어 필요 - MESI)
- **속도 빠름↑** 




